{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do some common imports, as well as Tensorflow itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6469596a45bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# To plot pretty figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's do a review of Tensorflow code itself. Tensorflow is usually not \"interactive\", in that the coding process is split into two parts. The first part consists of defining the graph to represent the desired mathematical computation. This static graph is stored, and is then ran and evaluated in a session after its definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a few variables, named x and y, and define a function using these two variables, to give a clearer example of the Tensorflow process. Tensorflow variables are defined with tf.Variable( ). Here x is initialized to 3 and y is initialized to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_16:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we defined our easy graph. Let's start a Tensorflow session to evaluate our function graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Logistic Regression in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, we will be using the MNIST dataset. We will use Tensorflow library to help us download the MNIST dataset to be used in our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define some of the constants we will be using in our model. Learning rate defines how quickly we update our parameters as we train our model. The epochs and batch size are parameters that affect the way our model trains with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "num_epochs = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some variables to be used in our model. The placeholders allow us to plug in values after defining our model. The variable starts off with values right away. In this case, we set the weights to random values, and we set the bias term to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal((784,10)))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then matrix multiply our input X with our weights W and add our bias b to get the raw output, aka logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the softmax function to the logits to get a probability for each output class. And then we use cross entropy to see how different this is from the expected output labels y. Tensorflow offers a function to do both of these steps in one go called softmax_cross_entropy_with_logits. We reduce this output array to its mean to represent the overall cost of our current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ask tensorflow to help us optimize our model using gradient descent by specifically minimizing the cost we had defined prior. We pass the learning rate into the optimizer, which is something we can mess around with to see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin training, we want to initialize all the variables we have defined. In addition, because of the nature of jupyter notebook splitting up chunks of code, we initialize a saver object to make sure that the model we are working with is the same in between code chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part of our Tensorflow development where we train our defined model. Essentially, we train our model \"num_epochs\" amount of times, going through the training set in set batch sizes. In each step, we run the optimizer and we feed our new batch of training data X and labels y into our model through feed_dict, thus training our model at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002 cost= 3.535523119\n",
      "Epoch: 0004 cost= 1.887914219\n",
      "Epoch: 0006 cost= 1.424355464\n",
      "Epoch: 0008 cost= 1.199291563\n",
      "Epoch: 0010 cost= 1.064394611\n",
      "Epoch: 0012 cost= 0.973408880\n",
      "Epoch: 0014 cost= 0.906925797\n",
      "Epoch: 0016 cost= 0.855853218\n",
      "Epoch: 0018 cost= 0.815264382\n",
      "Epoch: 0020 cost= 0.781937793\n",
      "Epoch: 0022 cost= 0.753793960\n",
      "Epoch: 0024 cost= 0.729667838\n",
      "Epoch: 0026 cost= 0.708727729\n",
      "Epoch: 0028 cost= 0.690246894\n",
      "Epoch: 0030 cost= 0.673882301\n",
      "Epoch: 0032 cost= 0.659124661\n",
      "Epoch: 0034 cost= 0.645728174\n",
      "Epoch: 0036 cost= 0.633557751\n",
      "Epoch: 0038 cost= 0.622394073\n",
      "Epoch: 0040 cost= 0.611986102\n",
      "Epoch: 0042 cost= 0.602515079\n",
      "Epoch: 0044 cost= 0.593564606\n",
      "Epoch: 0046 cost= 0.585288573\n",
      "Epoch: 0048 cost= 0.577487035\n",
      "Epoch: 0050 cost= 0.570046512\n",
      "Epoch: 0052 cost= 0.563230404\n",
      "Epoch: 0054 cost= 0.556666225\n",
      "Epoch: 0056 cost= 0.550502487\n",
      "Epoch: 0058 cost= 0.544609340\n",
      "Epoch: 0060 cost= 0.538958797\n",
      "Epoch: 0062 cost= 0.533597209\n",
      "Epoch: 0064 cost= 0.528562035\n",
      "Epoch: 0066 cost= 0.523697677\n",
      "Epoch: 0068 cost= 0.519026879\n",
      "Epoch: 0070 cost= 0.514482699\n",
      "Epoch: 0072 cost= 0.510104175\n",
      "Epoch: 0074 cost= 0.506093774\n",
      "Epoch: 0076 cost= 0.502094156\n",
      "Epoch: 0078 cost= 0.498274432\n",
      "Epoch: 0080 cost= 0.494603366\n",
      "Epoch: 0082 cost= 0.491064091\n",
      "Epoch: 0084 cost= 0.487546594\n",
      "Epoch: 0086 cost= 0.484144793\n",
      "Epoch: 0088 cost= 0.481031121\n",
      "Epoch: 0090 cost= 0.477842612\n",
      "Epoch: 0092 cost= 0.474840184\n",
      "Epoch: 0094 cost= 0.471905793\n",
      "Epoch: 0096 cost= 0.469080752\n",
      "Epoch: 0098 cost= 0.466328980\n",
      "Epoch: 0100 cost= 0.463604872\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: X_batch, y: y_batch})\n",
    "            avg_cost += c/total_batch\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    saver.save(sess, \"./saved_model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(two_d, interpolation='nearest', cmap='gray')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is just a function to help plot our MNIST images. Below we take in new test images in our model and predict the image. We then plot them to see visually how our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0092d0102eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./saved_model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_new_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_new_scaled\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./saved_model.ckpt\")\n",
    "    X_new_scaled = mnist.test.images\n",
    "    z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(z, axis=1)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels}))\n",
    "    for i in range(100):\n",
    "        if i%3 == 0:\n",
    "            gen_image(mnist.test.images[i]).show()\n",
    "            print(\"Predition: \", y_pred[i])\n",
    "            print(\"Actual label: \", np.argmax(mnist.test.labels[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
