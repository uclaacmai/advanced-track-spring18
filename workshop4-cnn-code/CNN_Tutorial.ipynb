{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Tutorial.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mzv10Zigp_X4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Hotdog or Not Hotdog?\n",
        "\n",
        "Today, we'll be training a Convolutional Neural Net to classify images\n",
        "\n",
        "The dataset we'll be using is at the link below. Be sure to click the \"Download all\" button. \n",
        "You may have log in to Kaggle. "
      ]
    },
    {
      "metadata": {
        "id": "IrMjh2OCM_Wc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog/data"
      ]
    },
    {
      "metadata": {
        "id": "o7gvl6565qtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Importing from Google Drive\n",
        "This code block uses PyDrive to import a file from your Google Drive storage\n",
        "onto Colab's local storage."
      ]
    },
    {
      "metadata": {
        "id": "THfCUuZSofOV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/hotdog_data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "\n",
        "############################################################################\n",
        "####### Have to replace this with a file ID from your Google Drive   #######\n",
        "\n",
        "file_id = '############################'\n",
        "\n",
        "###################### End of replacement portion ##########################\n",
        "############################################################################\n",
        "\n",
        "\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "\n",
        "file_lists = []\n",
        "file_list = drive.ListFile({'q': \"'{}' in parents\".format(file_id)}).GetList()\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrDlzm8grTiJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "##### We'll import all the Python modules we need\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import glob\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kbF4NvjRtMVr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Unzipping the file you've uploaded\n",
        "\n",
        "zip_path = '{}/hot-dog-not-hot-dog.zip'.format(local_download_path)\n",
        "zip_ref = zipfile.ZipFile('{}/hot-dog-not-hot-dog.zip'.format(local_download_path), 'r')\n",
        "zip_ref.extractall(local_download_path)\n",
        "zip_ref.close()\n",
        "zip_ref = zipfile.ZipFile('{}/seefood.zip'.format(local_download_path), 'r')\n",
        "zip_ref.extractall(local_download_path)\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQ2bkW4i7Cc6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Getting the dataset ready\n",
        "The data you got isn't ideally structured. There are around 500 images in the train data, and 500 images in the test data. Usually a distribution of 85% train and 15% test is adopted. Here, we've taken 900 images for the train data and 98 for the test. "
      ]
    },
    {
      "metadata": {
        "id": "owwvjbBDHZ9C",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_image_paths = [glob.glob('{}/test/hot_dog/*'.format(local_download_path)) , glob.glob('{}/test/not_hot_dog/*'.format(local_download_path))]\n",
        "train_image_paths = [glob.glob('{}/train/hot_dog/*'.format(local_download_path)) , glob.glob('{}/train/not_hot_dog/*'.format(local_download_path))]\n",
        "for i in range(201):\n",
        "  train_image_paths[0].append(test_image_paths[0][i])\n",
        "  train_image_paths[1].append(test_image_paths[1][i])\n",
        "test_image_paths[0] = test_image_paths[0][201:]\n",
        "test_image_paths[1] = test_image_paths[1][201:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t_R3Ja_i81Hf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now have a two lists, each with a number of paths to images. We need to extract the actual image data now. We'll use the \"io\" subpackage from the skimage module for that"
      ]
    },
    {
      "metadata": {
        "id": "afnOylraQXvU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# the 2 labels\n",
        "one_hot_vectors = np.array([[(lambda i,j: int(i == j))(i,j)  for i in range(2)] for j in range(2)])\n",
        "\n",
        "# function that takes a path to an image and extracts a 64 x 64 x 3 rescaled\n",
        "# version of the image\n",
        "\n",
        "def get_image(path):\n",
        "  pic = io.imread(path)\n",
        "  return resize(pic,(64,64))\n",
        "\n",
        "# function that takes a list of paths ((train/test)_image_paths)\n",
        "# and the number of images that are to be extracted (from each category\n",
        "# of images)\n",
        "\n",
        "def get_images(paths, max_imgs_per_type):\n",
        "  images = []\n",
        "  labels = []\n",
        "  img_ctrs = [0,0]\n",
        "  \n",
        "  max_ind = max_imgs_per_type\n",
        "  for i in range(2 * max_imgs_per_type):\n",
        "    ind = random.randint(0,1)\n",
        "    if(img_ctrs[ind] >= max_ind):\n",
        "      ind = int(not ind)   \n",
        "    image = get_image(paths[ind][img_ctrs[ind]])      \n",
        "    img_ctrs[ind] += 1\n",
        "    images.append(image)\n",
        "    labels.append(one_hot_vectors[ind])\n",
        "  return np.array(images),np.array(labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kW-dMlCouv-A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_images,train_labels = get_images(train_image_paths, 450)\n",
        "test_images, test_labels = get_images(test_image_paths, 49)\n",
        "\n",
        "train_images_len = len(train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gThM-mAwHCKF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# visualizing what our data looks like is helpful\n",
        "io.imshow(train_images[150])\n",
        "print(train_labels[150])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qwccpit5VOqx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def get_batch_and_labels(batch_size, starting_index, max_len):\n",
        "  batch = []\n",
        "  labels = []\n",
        "  \n",
        "  if starting_index + batch_size >= max_len:\n",
        "    batch = train_images[starting_index:]\n",
        "    labels = train_labels[starting_index:]\n",
        "    batch = np.concatenate((batch,train_images[:batch_size + starting_index - max_len]))\n",
        "    labels = np.concatenate((labels, train_labels[:batch_size + starting_index - max_len]))\n",
        "  else:\n",
        "    batch = train_images[starting_index:starting_index + batch_size]\n",
        "    labels = train_labels[starting_index:starting_index + batch_size]\n",
        "  return np.array(batch), np.array(labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "duDBeJjxC8F3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Our CNN model"
      ]
    },
    {
      "metadata": {
        "id": "EZqbm8aFv_XA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 250\n",
        "minibatch_size = 40\n",
        "lr = 1e-4\n",
        "keep = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4F7uKn9eZEX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# our input data\n",
        "x = tf.placeholder(tf.float32, shape = [None, 64,64 , 3])\n",
        "\n",
        "# our softmax predictions \n",
        "y_ = tf.placeholder(tf.float32, shape = [None, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpNQK2xdfDKP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(shape):\n",
        "    \"\"\"Initializes weights randomly from a normal distribution\n",
        "    Params: shape: list of dimensionality of the tensor to be initialized\n",
        "    \"\"\"\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    \"\"\"Initializes the bias term randomly from a normal distribution.\n",
        "    Params: shape: list of dimensionality for the bias term.\n",
        "    \"\"\"\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MG4eIJtDReNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Conv + Relu + Pool"
      ]
    },
    {
      "metadata": {
        "id": "wP2dAoxDfKMb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "W_conv1 = weight_variable([5, 5, 3, 16]) # 5 x 5 kernel (filter), across an image with 3 channels to 16 channels\n",
        "b_conv1 = bias_variable([16])\n",
        "h_conv1 = tf.nn.relu(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1) # conv + relu\n",
        "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1], padding='SAME') #max pool with 3x3 filter and stride 2\n",
        "h_pool1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2RJEwTXfsKZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Try it yourself !\n",
        "\n",
        "# We want a conv layer (followed by ReLu) with 5x5 filters \n",
        "# (we'll leave it to you to figure out the depth of the filters)\n",
        "# We also want the output to have a depth of 32\n",
        "# We want max pool filters of shape 3x3 that move with stride 2\n",
        "\n",
        "\n",
        "##### Begin inserting code here #####\n",
        "##### Four lines should suffice #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### End inserting code here #####\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-52M5vdhYEe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Try it yourself !\n",
        "\n",
        "# We want a conv layer (followed by ReLu) with 3x3 filters\n",
        "# (we'll leave it to you to figure out the depth of the filters)\n",
        "# We also want the output to have a depth of 64\n",
        "# We want max pool filters of shape 2x2 that move with stride 2\n",
        "\n",
        "\n",
        "##### Begin inserting code here #####\n",
        "##### Four lines should suffice #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### End inserting code here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2XN8h2HQudte",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Try it yourself !\n",
        "\n",
        "# We want a conv layer (followed by ReLu) with 3x3 filters, but here\n",
        "# we want the filters to move with stride 2 instead of the usual 1\n",
        "# (we'll leave it to you to figure out the depth of the filters)\n",
        "# We also want the output to have a depth of 64\n",
        "# We DON'T want a max pool layer\n",
        "\n",
        "# Make sure this code block runs without errors\n",
        "# (Hint: call the output of this layer h_conv4, \n",
        "# this just makes things easier for us)\n",
        "\n",
        "\n",
        "##### Begin inserting code here #####\n",
        "##### Three lines should suffice #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### End inserting code here #####\n",
        "h_conv4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "31yh1wMhRlG_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### FC + Dropout"
      ]
    },
    {
      "metadata": {
        "id": "cQZIA42QgLeJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# flattening h_conv4\n",
        "h_conv_flat = tf.reshape(h_conv4, [-1, 4 * 4 * 64])\n",
        "\n",
        "# Fully connected layer\n",
        "\n",
        "W_fc1 = weight_variable([4 * 4 * 64, 512]) \n",
        "b_fc1 = bias_variable([512])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_conv_flat, W_fc1) + b_fc1)\n",
        "\n",
        "# dropout layer\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMbfJauvgO7J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Try it yourself !\n",
        "\n",
        "# We want a FC layer that produces a vector of size 128\n",
        "\n",
        "##### Begin inserting code here #####\n",
        "##### Three lines should suffice #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### End inserting code here #####\n",
        "\n",
        "# We want a dropout layer with the same keep probability as before\n",
        "\n",
        "##### Begin inserting code here #####\n",
        "##### One line should suffice #####\n",
        "\n",
        "\n",
        "##### End inserting code here #####\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vPK0E6ZhPtC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Try it yourself !\n",
        "\n",
        "# This is our final FC layer \n",
        "# (we leave it to you to figure out what the size of the output vector should be)\n",
        "\n",
        "# This block should run without errors\n",
        "# (Hint: call the output of this layer y_out,\n",
        "# this just makes things easier for us)\n",
        "\n",
        "##### Begin inserting code here #####\n",
        "##### Three lines should suffice #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### End inserting code here #####\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hPPPUrXtRq_5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Loss"
      ]
    },
    {
      "metadata": {
        "id": "lHNTkLRIhTi_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = y_out, labels = y_)\n",
        "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "correct_prediction = tf.equal(tf.argmax(y_, axis = 1), tf.argmax(y_out, axis = 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFbFhsTNRuJ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Training"
      ]
    },
    {
      "metadata": {
        "id": "Q0CvtgnVicz3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for i in range(n_epochs):\n",
        "    training_inputs, training_labels = get_batch_and_labels(minibatch_size, i*minibatch_size, train_images_len)    \n",
        "    if i % 1 == 0:\n",
        "      print(\"epoch:{}\".format(i))\n",
        "      train_acc = accuracy.eval(feed_dict = {x: training_inputs, y_: training_labels, keep_prob : 1.0})\n",
        "      print(\"training accuracy: {}\".format(train_acc))\n",
        "    \n",
        "    ##### sess.run #####\n",
        "    #### Start code insertion (careful with indentation) ####\n",
        "    \n",
        "    \n",
        "    ################## End code insertion ###################\n",
        "  \n",
        "  #### test_acc ####\n",
        "  #### Start code insertion (careful with indentation) ####\n",
        "  \n",
        "  \n",
        "  ################ End code insertion #####################\n",
        "  print(\"test accuracy: {}\".format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hgY4e-0mQXiO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Results\n",
        "\n",
        "You'll see our model achieved a good training accuracy but a pretty bad test accuracy.\n",
        "\n",
        "Why is our model doing badly with test data?\n",
        "\n",
        "What are the remedies ?\n"
      ]
    },
    {
      "metadata": {
        "id": "KQsg5qJKQ0Io",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##More resources:\n",
        "\n",
        "http://cs231n.github.io/convolutional-networks/\n"
      ]
    }
  ]
}