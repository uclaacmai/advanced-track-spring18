{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks with TensorFlow\n",
    "## Using the MNIST dataset (...again!)\n",
    "\n",
    "\n",
    "### Review: Neurons, Perceptrons, Logistic Regression...\n",
    "They're all the same thing! Neural networks are essentially a fully-connected* network of units commonly referred to as the perceptron. It performs logistic regression as we've learned it in the previous workshop, and looks a little something like this:\n",
    "<img src = perceptron.png>\n",
    "note: just realized we're using collab so you won't be able to see the images... just look up plz\n",
    "\n",
    "In particular, note that the inputs are each multiplied by as respective weights to give a weighted sum. In our implementation, we will have an additional bias term added to the sum as well. Biases are important in helping learn the optimal mapping function -- if we don't have the bias term, we limit the function to pass through the origin.\n",
    "\n",
    "In addition, the step function in the image is what we have been referring to as the activation function. The weighted sum is passed through the activation function which gives us our final output.\n",
    "\n",
    "This can be expressed in the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "h(x) = a(Wx + b)\n",
    "\\end{equation*}\n",
    "\n",
    "where $a$ is the activation function, $W$ is the weight vector, $x$ is the input vector, and $b$ is the bias term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports from last workshop\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and our architecture\n",
    "\n",
    "These can be thought of as variables that \"tune\" our models. These parameters define the overall architecture of the neural network. Picking the right hyperparameters is tricky and there's no theoretical \"best\" (although efforts have been made to find some, such as Google's AutoML).\n",
    "\n",
    "This is the architecture that we want to implement:\n",
    "<img src = hiddenlayers.gif>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5000\n",
    "batch_size = 100\n",
    "\n",
    "hidden_units1 = 100\n",
    "hidden_units2 = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data placeholder\n",
    "\n",
    "This is where we promise the model a certain dataset as input of a particular shape.\n",
    "\n",
    "Hint: for MNIST, the images are 28x28 pixels and there are 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data placeholders\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "# y_ = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hidden layers \n",
    "\n",
    "In this section we define variables that hold the weights for our model. In particular, pay attention to what shape they should be, as well as our options for initialization.\n",
    "\n",
    "Hint: tf.Variable, tf.truncated_normal, tf.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hidden layers\n",
    "\n",
    "# input -> hidden layer 1\n",
    "W1 = tf.Variable(tf.truncated_normal([784, hidden_units1], stddev=0.1))\n",
    "# B1 = ...\n",
    "\n",
    "# hidden layer 1 -> hidden layer 2\n",
    "# W2 = ... \n",
    "# B2 = ... \n",
    "\n",
    "# hidden layer 2 -> output layer\n",
    "# W3 = ...\n",
    "# B3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Activation functions and the activation layer\n",
    "Sigmoid \n",
    "<img src = sigmoid.png>\n",
    "Tanh\n",
    "<img src = tanh.png>\n",
    "ReLu\n",
    "<img src = relu.jpeg>\n",
    "\n",
    "Recall our discussion of activation functions (sigmoid, tanh, relu, softmax, etc...). Before, our models used them to give us a classification for a specific input. In neural networks, the gain a second use by providing **nonlinearity** after every hidden layer. \n",
    "\n",
    "From last workshop, we discussed the **universal approximation theorem**, which states that a sufficient number of nonlinear hidden units can approximate any continuous function. The nonlinearity from the activation functions allow us to accomplish just that with neural networks. \n",
    "\n",
    "Going back to the first usage of the activation function -- classification, we look at the **softmax function**. The softmax function is a generalization of the sigmoid function. The sigmoid function is useful for a 2-class classifier. In the classic example of classifying tumors, the sigmoid function will give us a 1 or a 0 for a malignant or benign tumor -- 2 classes! For use with the MNIST dataset, we are classifying an image as a number from 0-9, so we need 10 classes. \n",
    "Here's a visual for softmax: \n",
    "<img src = softmax.png>\n",
    "We can intepret the final output vector as the probability of the input data being in a particular class. They add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hidden layers (they're essentially functions!)\n",
    "\n",
    "Perceptron equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "h(x) = a(Wx + b)\n",
    "\\end{equation*}\n",
    "\n",
    "where $a$ is the activation function, $W$ is the weight vector, $x$ is the input vector, and $b$ is the bias term.\n",
    "\n",
    "Hint: tf.nn.relu, tf.matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hidden layers\n",
    "\n",
    "# hidden layer 1 output\n",
    "# H1 = ...\n",
    "\n",
    "# hidden layer 2 output\n",
    "# H2 = ...\n",
    "\n",
    "# output layer output (before softmax)\n",
    "# logits = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track accuracy as network trains\n",
    "correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run our model!\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        optimizer.run(feed_dict = {x: x_batch, y_: y_batch})\n",
    "        if i % 100 == 0:\n",
    "            acc = accuracy.eval(feed_dict = {x: x_batch, y_: y_batch})\n",
    "            loss = cross_entropy_loss.eval(feed_dict = {x: x_batch, y_: y_batch})\n",
    "            print(\"Epoch: {}, accuracy: {}, loss: {}\".format(i, acc, loss))\n",
    "\n",
    "    acc = accuracy.eval(feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
    "    print(\"Test accuracy: {}\".format(acc))\n",
    "    saver.save(sess, \"./saved_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(two_d, interpolation='nearest', cmap='gray')\n",
    "    return plt\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./saved_model.ckpt\")\n",
    "    X_new_scaled = mnist.test.images\n",
    "    z = logits.eval(feed_dict={x: X_new_scaled})\n",
    "    y_pred = np.argmax(z, axis=1)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "    for i in range(100):\n",
    "        if i%3 == 0:\n",
    "            gen_image(mnist.test.images[i]).show()\n",
    "            print(\"Predition: \", y_pred[i])\n",
    "            print(\"Actual label: \", np.argmax(mnist.test.labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
