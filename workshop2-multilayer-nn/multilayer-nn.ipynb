{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks with TensorFlow\n",
    "## Using the MNIST dataset (...again!)\n",
    "\n",
    "\n",
    "### Review: Neurons, Perceptrons, Logistic Regression...\n",
    "They're all the same thing! Neural networks are essentially a fully-connected* network of units commonly reffered to as the perceptron. It performs logistic regression as we've learned it in the previous workshop, and looks a little something like this:\n",
    "<img src = perceptron.png>\n",
    "note: just realized we're using collab so you won't be able to see the images... just look up plz\n",
    "\n",
    "In particular, note that the inputs are each multiplied by as respective weights to give a weighted sum. In our implementation, we will have an additional bias term added to the sum as well. Biases are important in helping learn the optimal mapping function -- if we don't have the bias term, we limit the function to pass through the origin.\n",
    "\n",
    "In addition, the step function in the image is what we have been referring to as the activation function. The weighted sum is passed through the activation function which gives us our final output.\n",
    "\n",
    "This can be expressed in the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "h(x) = a(Wx + b)\n",
    "\\end{equation*}\n",
    "\n",
    "where $a$ is the activation function, $W$ is the weight vector, $x$ is the input vector, and $b$ is the bias term.\n",
    "\n",
    "### Review: Activation functions and the activation layer\n",
    "Sigmoid \n",
    "<img src = sigmoid.png>\n",
    "Tanh\n",
    "<img src = tanh.png>\n",
    "ReLu\n",
    "<img src = relu.jpeg>\n",
    "\n",
    "Recall our discussion of activation functions (sigmoid, tanh, relu, softmax, etc...). Before, our models used them to give us a classification for a specific input. In neural networks, the gain a second use by providing **nonlinearity** after every hidden layer. \n",
    "\n",
    "From last workshop, we discussed the **universal approximation theorem**, which states that a sufficient number of nonlinear hidden units can approximate any continuous function. The nonlinearity from the activation functions allow us to accomplish just that with neural networks. \n",
    "\n",
    "Going back to the first usage of the activation function -- classification, we look at the **softmax function**. The softmax function is a generalization of the sigmoid function. The sigmoid function is useful for a 2-class classifier. In the classic example of classifying tumors, the sigmoid function will give us a 1 or a 0 for a malignant or beign tumor -- 2 classes! For use with the MNIST dataset, we are classifying an image as a number from 0-9, so we need 10 classes. \n",
    "Here's a visual for softmax: \n",
    "<img src = softmax.png>\n",
    "We can intepret the final output vector as the probability of the input data being in a particular class. They add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Standard imports from last workshop\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5000\n",
    "batch_size = 100\n",
    "\n",
    "hidden_units1 = 100\n",
    "hidden_units2 = 35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data placeholders\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hidden layers\n",
    "\n",
    "# input -> hidden layer 1\n",
    "W1 = tf.Variable(tf.truncated_normal([784, hidden_units1], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [hidden_units1])\n",
    "\n",
    "# hidden layer 1 -> hidden layer 2\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden_units1, hidden_units2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1), [hidden_units2])\n",
    "\n",
    "# hidden layer 2 -> output layer\n",
    "W3 = tf.Variable(tf.truncated_normal([hidden_units2, 10], stddev=0.1))\n",
    "B3 = tf.Variable(tf.constant(0.1), [10])\n",
    "\n",
    "# hidden layer 1 output\n",
    "H1 = tf.nn.relu(tf.matmul(x, W1) + B1)\n",
    "\n",
    "# hidden layer 2 output\n",
    "H2 = tf.nn.relu(tf.matmul(H1, W2) + B2)\n",
    "\n",
    "# output layer output (before softmax)\n",
    "logits = tf.matmul(H2, W3) + B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track accuracy as network trains\n",
    "correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run our model!\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, accuracy: 0.20999999344348907, loss: 2.249713182449341\n",
      "Epoch: 100, accuracy: 0.8199999928474426, loss: 0.5885079503059387\n",
      "Epoch: 200, accuracy: 0.8999999761581421, loss: 0.3585788309574127\n",
      "Epoch: 300, accuracy: 0.9800000190734863, loss: 0.1531728357076645\n",
      "Epoch: 400, accuracy: 0.9300000071525574, loss: 0.3084542155265808\n",
      "Epoch: 500, accuracy: 0.9399999976158142, loss: 0.24728238582611084\n",
      "Epoch: 600, accuracy: 0.9599999785423279, loss: 0.21088962256908417\n",
      "Epoch: 700, accuracy: 0.9200000166893005, loss: 0.22191591560840607\n",
      "Epoch: 800, accuracy: 0.9300000071525574, loss: 0.31745007634162903\n",
      "Epoch: 900, accuracy: 0.9399999976158142, loss: 0.18803171813488007\n",
      "Epoch: 1000, accuracy: 0.9599999785423279, loss: 0.18146780133247375\n",
      "Epoch: 1100, accuracy: 0.9599999785423279, loss: 0.09576275944709778\n",
      "Epoch: 1200, accuracy: 0.9599999785423279, loss: 0.1341056078672409\n",
      "Epoch: 1300, accuracy: 0.9800000190734863, loss: 0.07908398658037186\n",
      "Epoch: 1400, accuracy: 0.9399999976158142, loss: 0.20944525301456451\n",
      "Epoch: 1500, accuracy: 0.9300000071525574, loss: 0.19839239120483398\n",
      "Epoch: 1600, accuracy: 0.9200000166893005, loss: 0.1359284371137619\n",
      "Epoch: 1700, accuracy: 0.9900000095367432, loss: 0.06389129906892776\n",
      "Epoch: 1800, accuracy: 0.9700000286102295, loss: 0.1352665275335312\n",
      "Epoch: 1900, accuracy: 0.9599999785423279, loss: 0.12722615897655487\n",
      "Epoch: 2000, accuracy: 0.9700000286102295, loss: 0.090105801820755\n",
      "Epoch: 2100, accuracy: 0.9900000095367432, loss: 0.052125997841358185\n",
      "Epoch: 2200, accuracy: 0.9700000286102295, loss: 0.10240970551967621\n",
      "Epoch: 2300, accuracy: 0.9900000095367432, loss: 0.04397069290280342\n",
      "Epoch: 2400, accuracy: 0.9900000095367432, loss: 0.07337634265422821\n",
      "Epoch: 2500, accuracy: 0.9800000190734863, loss: 0.0953434631228447\n",
      "Epoch: 2600, accuracy: 0.9700000286102295, loss: 0.06876789778470993\n",
      "Epoch: 2700, accuracy: 0.9800000190734863, loss: 0.04421673342585564\n",
      "Epoch: 2800, accuracy: 0.949999988079071, loss: 0.12130389362573624\n",
      "Epoch: 2900, accuracy: 1.0, loss: 0.03831113502383232\n",
      "Epoch: 3000, accuracy: 0.9900000095367432, loss: 0.0791071206331253\n",
      "Epoch: 3100, accuracy: 1.0, loss: 0.026419920846819878\n",
      "Epoch: 3200, accuracy: 0.9800000190734863, loss: 0.08358048647642136\n",
      "Epoch: 3300, accuracy: 0.9599999785423279, loss: 0.12450339645147324\n",
      "Epoch: 3400, accuracy: 0.9599999785423279, loss: 0.12295641005039215\n",
      "Epoch: 3500, accuracy: 1.0, loss: 0.02838658168911934\n",
      "Epoch: 3600, accuracy: 0.9900000095367432, loss: 0.06454842537641525\n",
      "Epoch: 3700, accuracy: 0.9900000095367432, loss: 0.0456429198384285\n",
      "Epoch: 3800, accuracy: 1.0, loss: 0.03314012661576271\n",
      "Epoch: 3900, accuracy: 0.9900000095367432, loss: 0.052550893276929855\n",
      "Epoch: 4000, accuracy: 1.0, loss: 0.02375689335167408\n",
      "Epoch: 4100, accuracy: 0.9900000095367432, loss: 0.03362506255507469\n",
      "Epoch: 4200, accuracy: 0.9900000095367432, loss: 0.04311051964759827\n",
      "Epoch: 4300, accuracy: 1.0, loss: 0.02971849963068962\n",
      "Epoch: 4400, accuracy: 0.9900000095367432, loss: 0.035742733627557755\n",
      "Epoch: 4500, accuracy: 1.0, loss: 0.025387901812791824\n",
      "Epoch: 4600, accuracy: 0.9900000095367432, loss: 0.056755077093839645\n",
      "Epoch: 4700, accuracy: 0.9599999785423279, loss: 0.07540614902973175\n",
      "Epoch: 4800, accuracy: 1.0, loss: 0.021104909479618073\n",
      "Epoch: 4900, accuracy: 0.9900000095367432, loss: 0.0541393905878067\n",
      "Test accuracy: 0.972100019454956\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        optimizer.run(feed_dict = {x: x_batch, y_: y_batch})\n",
    "        if i % 100 == 0:\n",
    "            acc = accuracy.eval(feed_dict = {x: x_batch, y_: y_batch})\n",
    "            loss = cross_entropy_loss.eval(feed_dict = {x: x_batch, y_: y_batch})\n",
    "            print(\"Epoch: {}, accuracy: {}, loss: {}\".format(i, acc, loss))\n",
    "\n",
    "    acc = accuracy.eval(feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
    "    print(\"Test accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
