{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ko0tAffRrvBy"
   },
   "source": [
    "# Convolutional Neural Network and MNIST\n",
    "\n",
    "In this tutorial, we'll be walking through the Tensorflow code behind creating a convolutional neural network. If you'd like more of a conceptual view of how these networks work, check out my [this blog post by Adit](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/). A CNN tutorial from the Tensorflow docs can also be found [here](https://www.tensorflow.org/tutorials/deep_cnn).\n",
    "\n",
    "A large part of the coding today will be keeping track of the dimensionality of the inputs, the outputs, the filters, and how they change as we set up the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RlM8Cb1VgFVi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6450,
     "status": "ok",
     "timestamp": 1526018465672,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "7CfLyWI6geyb",
    "outputId": "49b559ac-b747-4d13-b109-d8bed74d5ab2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-Ed8RHkPxqNj"
   },
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this next step, x and y_ act as placeholders that basically just indicate the type of input you want in your CNN and the type of output. For each of these placeholders, you have to specify the type and the shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zP279pCtgtND"
   },
   "outputs": [],
   "source": [
    "# defining placeholders\n",
    "x = tf.placeholder(\"float\", shape = [None, 28, 28 , 1])\n",
    "y_ = tf.placeholder(\"float\", shape = [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "op-rFWhTnWg1"
   },
   "source": [
    "### Define some helper functions\n",
    "We like clean code, so we're going to define functions to make it easier when we need them. The function will be for the convolutions and the pooling layers.\n",
    "\n",
    "Now keep something in mind: the 4-D input is of the format [batch, in_height, in_width, in_depth], and the 4-D filter is of the format [filter_height, filter_width, filter_depth, count]\n",
    "\n",
    "(here, count is the number of filters, batch is usually 1 in any single iteration of the convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZY4iLf69mzJc"
   },
   "outputs": [],
   "source": [
    "# padding = 1, strides = 1\n",
    "# TODO = fill in strides[]\n",
    "def conv2d(x, w):\n",
    "  return tf.nn.conv2d(input=x, filter=w, strides=[...], padding=\"SAME\")\n",
    "\n",
    "# padding = none, strides = 2, filter size = 2\n",
    "# note that the pool layer isn't parameterized by weights\n",
    "# TODO = fill in the ksize[] and strides[]\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[...], strides=[...], padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture:\n",
    "\n",
    "INPUT -> [CONV1 -> RELU1 -> POOL1] -> [CONV2 -> RELU2 -> POOL2] -> FC1 -> FC2 -> (SOFTMAX, OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxzsP-ZvrkfT"
   },
   "source": [
    "### Define the first Conv-Relu-Pool (Convolutional) layers\n",
    "\n",
    "Now we get to the good stuff: the conv layers. Here, we have selected the number of filters used will be 32. This is also a hyper parameter. The weights here (w_conv1) is the literally the weight of each filter, and since we're using 32 filters, the weight matrix will be of volume 3x3x32. We're going to be initilizing the values at random from a normal distribution with stddev=0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wV-9Y0hfhqaQ"
   },
   "outputs": [],
   "source": [
    "# TODO = fill in the tf.truncated_normal distribution matrix sizes\n",
    "w_conv1 = tf.Variable(tf.truncated_normal([...], stddev=0.1))\n",
    "\n",
    "# TODO = fill in bias variable shape\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape = [...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1526018489774,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "8gilkyUXlDea",
    "outputId": "b50b3c5e-bca2-4923-c0cd-ed7a363f054e"
   },
   "outputs": [],
   "source": [
    "# TODO = fill in arguements for the pre-defined functions\n",
    "h_conv1 = conv2d(...)\n",
    "h_relu1 = tf.nn.relu(...)\n",
    "h_pool1 = max_pool_2x2(...)\n",
    "\n",
    "print(h_conv1)\n",
    "print(h_relu1)\n",
    "print(h_pool1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbB-sgcdrqUh"
   },
   "source": [
    "### Define the second Conv-Relu-Pool (Convolutional) layers\n",
    "In our second Convolution layer, we're going to increase the number of filters from to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RShLEzYOrS9k"
   },
   "outputs": [],
   "source": [
    "# TODO = fill in the tf.truncated_normal distribution matrix sizes\n",
    "w_conv2 = tf.Variable(tf.truncated_normal([...], stddev=0.1))\n",
    "\n",
    "# TODO = fill in bias variable shape\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1526018491209,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "JS7Xm4lRs7-s",
    "outputId": "730a2c23-faa2-4b0e-f136-3fd735238c41"
   },
   "outputs": [],
   "source": [
    "h_conv2 = conv2d(h_pool1, w_conv2)\n",
    "h_relu2 = tf.nn.relu(h_conv2)\n",
    "h_pool2 = max_pool_2x2(h_relu2)\n",
    "\n",
    "print(h_conv2)\n",
    "print(h_relu2)\n",
    "print(h_pool2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcFu4L5-tO3w"
   },
   "source": [
    "### Define the first FC layer\n",
    "Aaaaaaaaand we step back into more familiar territory. Here, we start working on our fully connected layers, like the ones we did last workshop. We're going to have another hyperparameter here, it beinng the number of hidden units of perceptrons per layer, which we've decided as 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1526018491994,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "y8BeIMYotNFz",
    "outputId": "5af8a2c0-dfbf-4137-8c5e-78df870f69fa"
   },
   "outputs": [],
   "source": [
    "fc_units1 = 1024\n",
    "\n",
    "# TODO = fill in the size of the weights matrix      [hint: we're trying to consolidate the whole of the 3-D input into 1-D]\n",
    "w_fc1 = tf.Variable(tf.truncated_normal([..., fc_units1], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[fc_units1]))\n",
    "\n",
    "# TODO = fill in the size of the input matrix         [hint: it's the same value as above]\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, ...])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "print(h_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ihzsn-Itv52h"
   },
   "source": [
    "### Define the Output Layer\n",
    "\n",
    "Remember the output layer is returning the class probabilities (10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1526018492855,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "AubqFTJYvCMf",
    "outputId": "b1796987-e59b-48e4-928a-4f97b700b555"
   },
   "outputs": [],
   "source": [
    "w_fc2 = tf.Variable(tf.truncated_normal([fc_units1, 10], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "logits = tf.matmul(h_fc1, w_fc2) + b_fc2\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_E5JZf6twb0M"
   },
   "source": [
    "### Softmax! Lost Function! Optimization!\n",
    "And now starts our backpropogation. We're trying to find how accurate we were to the labels (Loss function using cross entorpy loss), and we're training our optimizer to modify the weights in the most ideal way to give the best accuracy the next iteration. (Though we aren't changing the weights just yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1526018494095,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "wt3CFuhswYHr",
    "outputId": "6385c89d-9ef7-458d-ee6c-9cafbbb7b98d"
   },
   "outputs": [],
   "source": [
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1MQrGJKxJNA"
   },
   "source": [
    "### Some metrics to track while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "U5Ws7Ysmw71n"
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7SthjZDxV2K"
   },
   "source": [
    "### Let's train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0aui1JzdxRE9"
   },
   "outputs": [],
   "source": [
    "# initialize tensorflow variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37722,
     "status": "ok",
     "timestamp": 1526018653446,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "k7YaZ2YRxeot",
    "outputId": "5fa0fb30-161a-4aa7-a491-98bbbd452b3e"
   },
   "outputs": [],
   "source": [
    "# run model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        x_batch = x_batch.reshape([batch_size, 28, 28, 1]) # we no longer want a flat vector of pixels\n",
    "        optimizer.run(feed_dict = {x: x_batch, y_: y_batch})\n",
    "        if i % 100 == 0:\n",
    "            acc = accuracy.eval(feed_dict = {x: x_batch, y_: y_batch})\n",
    "            loss = cross_entropy_loss.eval(feed_dict = {x: x_batch, y_: y_batch})\n",
    "            print(\"Epoch: {}, accuracy: {}, loss: {}\".format(i, acc, loss))\n",
    "\n",
    "    acc = accuracy.eval(feed_dict = {x: mnist.test.images.reshape([-1, 28, 28, 1]), y_:mnist.test.labels}) # here too\n",
    "    print(\"Test accuracy: {}\".format(acc))\n",
    "    saver.save(sess, \"./saved_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woZHHjOlz9IX"
   },
   "source": [
    "### Print out some pictures like we did for the previous workshops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 9639
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6472,
     "status": "ok",
     "timestamp": 1526018927339,
     "user": {
      "displayName": "Henry Yang",
      "photoUrl": "//lh6.googleusercontent.com/-GXPDq6tqJwc/AAAAAAAAAAI/AAAAAAAAAuQ/rfqB3nVANgo/s50-c-k-no/photo.jpg",
      "userId": "103780741221149720480"
     },
     "user_tz": 420
    },
    "id": "Ksnh8bgQx2ap",
    "outputId": "38273c38-ecc1-4378-c2a7-8b9f5d426cd6"
   },
   "outputs": [],
   "source": [
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(two_d, interpolation='nearest', cmap='gray')\n",
    "    return plt\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./saved_model.ckpt\")\n",
    "    X_new_scaled = mnist.test.images\n",
    "    z = logits.eval(feed_dict={x: X_new_scaled.reshape([-1, 28, 28, 1])})\n",
    "    y_pred = np.argmax(z, axis=1)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(accuracy.eval(feed_dict={x: mnist.test.images.reshape([-1, 28, 28, 1]), y_: mnist.test.labels}))\n",
    "    for i in range(100):\n",
    "        if i%3 == 0:\n",
    "            gen_image(mnist.test.images[i]).show()\n",
    "            print(\"Predition: \", y_pred[i])\n",
    "            print(\"Actual label: \", np.argmax(mnist.test.labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "cnn_mnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
